import gradio as gr
import random
from llama_cpp import Llama
from pywhispercpp.model import Model

# å®šä¹‰æ¨¡å‹åç§°å’Œè·¯å¾„çš„æ˜ å°„
model_paths = {
    "Qwen2.5-3B-Instruct-Q4_0_4_4": "./Qwen2.5-3B-Instruct-Q4_0_4_4.gguf",
    "Qwen2.5-7B-Instruct-Q4_0_4_4": "./Qwen2.5-7B-Instruct-Q4_0_4_4.gguf",
}
# åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥å¤„ç†æ¨¡å‹é€‰æ‹©ï¼Œå¹¶è¿”å›æ¨¡å‹è·¯å¾„
def get_model_path(selected_model_name):
    return model_paths[selected_model_name]

model_path = "./Qwen2.5-3B-Instruct-Q4_0_4_4.gguf"  # é»˜è®¤llmæ¨¡å‹è·¯å¾„

#  ä½¿ç”¨ whisper.cpp
# å…¨å±€å˜é‡
whisper_model = None

def load_model():
    global whisper_model
    if whisper_model is None:
        whisper_model = Model('./ggml-small-q4_0.bin')
# audio to text here
def audio_to_text(audio_path):
    """
    audio to text hereï¼Œç›®å‰æ˜¯ whisper.cpp
    Parameters:
    audio_path: str, éŸ³é¢‘æ–‡ä»¶è·¯å¾„
    Returns:
    transcription.text: str, éŸ³é¢‘è½¬æ¢çš„æ–‡æœ¬
    """
    load_model()  # ç¡®ä¿æ¨¡å‹åªåŠ è½½ä¸€æ¬¡
    if not audio_path:
        return None
    print(f"æ­£åœ¨å¤„ç†audio_path:{audio_path}")
    params = {
        'n_threads': 8,
        'language': 'auto',  
        'initial_prompt': 'ä»¥ä¸‹æ˜¯æ™®é€šè¯çš„å¥å­',
    }
    result = whisper_model.transcribe(audio_path, **params)
    text = ''.join(segment.text for segment in result)
    print("Transcription text:", text)
    return text



class LLMResponse:
    def __init__(self, model_path):
        self.llm = Llama(
            model_path=model_path,
            chat_format="chatml",
            n_ctx=1024,
            n_threads=8,
        )

    def get_response(self, messages):
        # åˆ›å»ºèŠå¤©å®Œæˆ
        response = self.llm.create_chat_completion(
            messages=messages,
            stream=True,
            max_tokens=512,
            top_p=0.8,  # é‡‡æ ·æ¦‚ç‡é˜ˆå€¼
            top_k=100,  # é‡‡æ ·æ•°é‡
            temperature=0.7,  # æ¸©åº¦
        )
        # æµå¼è¿”å›AIçš„å“åº”
        for chunk in response:
            if "choices" in chunk and chunk["choices"]:
                delta = chunk["choices"][0].get("delta", {})
                content = delta.get("content", "")
                if content:
                    yield content

def chat_completions(messages, gr_states, history, selected_model_name):
    """
    Chat completion here, using llama-cpp-python now.
    Parameters:
    messages: openai format messages
    gr_states: dict, global states
    history: list, conversation history
    Returns:
    gr_states: dict, updated global states
    history: list, updated conversation history
    """
    if not messages:
        return gr_states, history
    
    # Initialize the LLM model
    model_path = get_model_path(selected_model_name)  # æ ¹æ®æ¨¡å‹åç§°è·å–è·¯å¾„
    llm_response = LLMResponse(model_path)
    
    # æ„å»ºåŒ…å«æ•´ä¸ªå¯¹è¯å†å²çš„æ¶ˆæ¯åˆ—è¡¨
    full_messages = [
        {"role": "system", "content": gr_states["system_prompt"]},
    ]
    # æ·»åŠ å†å²å¯¹è¯åˆ°æ¶ˆæ¯åˆ—è¡¨ä¸­
    for usr, bot in history:
        full_messages.append({"role": "user", "content": usr})
        if bot:
            full_messages.append({"role": "assistant", "content": bot})
    
    # æ·»åŠ æœ€æ–°çš„ç”¨æˆ·è¾“å…¥
    full_messages.append({"role": "user", "content": messages[-1]['content']})
    
    # Get the response from the LLM model
    ai_response = ""
    for content in llm_response.get_response(full_messages):
        ai_response += content
        gr_states["history"][-1][1] = ai_response  # æ›´æ–°å†å²è®°å½•
        history[-1][1] = ai_response  # æ›´æ–° history
        yield gr_states, history  # é€æ­¥è¿”å›æ›´æ–°åçš„ gr_states å’Œ history
    
    # Update the conversation history
    user_input = messages[-1]['content']
    if user_input:
        if not gr_states["history"]:
            gr_states["history"] = []
        gr_states["history"].append([user_input, ai_response])
        history.pop()
        history.append(gr_states["history"][-1])
    
    yield gr_states, history



def init_default_role():
    """
    åˆå§‹åŒ–é»˜è®¤è§’è‰²
    æ ¹æ®è§’è‰²ç¡®å®š system prompt
    """
    system_prompt = "ä½ æ˜¯ä¸€åªä¼šè¯´è¯çš„é’è›™ï¼Œä½†æ— è®ºè¯´ä»€ä¹ˆéƒ½çˆ±åœ¨æœ€ååŠ ä¸Š'å‘±å”§å‘±å”§'ã€‚"
    role = "ä¸€åªç”¨äºæ¼”ç¤ºçš„é’è›™ ğŸ¸"
    role_description = "æˆ‘æ˜¯ä¸€åªä¼šè¯´è¯çš„é’è›™ğŸ¸ï¼Œä½†æ— è®ºè¯´ä»€ä¹ˆéƒ½çˆ±åœ¨æœ€ååŠ ä¸Š'å‘±å”§å‘±å”§'ã€‚"
    return role, role_description, system_prompt

def get_random_role():
    """
    éšæœºè·å–ä¸€ä¸ªè§’è‰²ï¼Œè¿™é‡Œåªæ˜¯ä¸€ä¸ªç¤ºä¾‹å‡½æ•°
    æ ¹æ®è§’è‰²ç¡®å®š system prompt
    """
    roles = [
        {
            "name": "0å·å°é’è›™ ğŸ¸",
            "description": "æˆ‘æ˜¯ä¸€åªä¼šè¯´è¯çš„é’è›™ğŸ¸ï¼Œä½†æ— è®ºè¯´ä»€ä¹ˆéƒ½çˆ±åœ¨æœ€ååŠ ä¸Š'å‘±å”§å‘±å”§'ã€‚",
            "system_prompt": "ä½ æ˜¯ä¸€åªä¼šè¯´è¯çš„é’è›™ï¼Œä½†æ— è®ºè¯´ä»€ä¹ˆéƒ½çˆ±åœ¨æœ€ååŠ ä¸Š'å‘±å”§å‘±å”§'ã€‚"
        },
        {
            "name": "1å·å°è€è™ ğŸ¯",
            "description": "æˆ‘æ˜¯ä¸€åªå‹‡æ•¢çš„å°è€è™ğŸ¯ï¼Œè¯´è¯æ—¶æ€»æ˜¯å……æ»¡åŠ›é‡ï¼Œå–œæ¬¢åœ¨å¥å­æœ«å°¾åŠ ä¸Š'å—·å‘œå—·å‘œ'ã€‚",
            "system_prompt": "ä½ æ˜¯ä¸€åªå‹‡æ•¢çš„å°è€è™ï¼Œè¯´è¯æ—¶æ€»æ˜¯å……æ»¡åŠ›é‡ï¼Œå–œæ¬¢åœ¨å¥å­æœ«å°¾åŠ ä¸Š'å—·å‘œå—·å‘œ'ã€‚"
        },
        {
            "name": "2å·å°çŒ«å’ª ğŸ±",
            "description": "æˆ‘æ˜¯ä¸€åªæ¸©æŸ”çš„å°çŒ«å’ªğŸ±ï¼Œè¯´è¯æ—¶å£°éŸ³æŸ”å’Œï¼Œå–œæ¬¢åœ¨å¥å­æœ«å°¾åŠ ä¸Š'å–µå–µå–µå–µ'ã€‚",
            "system_prompt": "ä½ æ˜¯ä¸€åªæ¸©æŸ”çš„å°çŒ«å’ªï¼Œè¯´è¯æ—¶å£°éŸ³æŸ”å’Œï¼Œå–œæ¬¢åœ¨å¥å­æœ«å°¾åŠ ä¸Š'å–µå–µå–µå–µ'ã€‚"
        },
        {
            "name": "3å·å°æ°´ç‰› ğŸ®",
            "description": "æˆ‘æ˜¯ä¸€åªå‹¤åŠ³çš„å°æ°´ç‰›ğŸ®ï¼Œè¯´è¯æ—¶æ€»æ˜¯æ…¢æ¡æ–¯ç†ï¼Œå–œæ¬¢åœ¨å¥å­æœ«å°¾åŠ ä¸Š'å“å“å“å“'ã€‚",
            "system_prompt": "ä½ æ˜¯ä¸€åªå‹¤åŠ³çš„å°æ°´ç‰›ï¼Œè¯´è¯æ—¶æ€»æ˜¯æ…¢æ¡æ–¯ç†ï¼Œå–œæ¬¢åœ¨å¥å­æœ«å°¾åŠ ä¸Š'å“å“å“å“'ã€‚"
        },
        {
            "name": "4å·å°ç‹—ç‹— ğŸ¶",
            "description": "æˆ‘æ˜¯ä¸€åªå¿ è¯šçš„å°ç‹—ç‹—ğŸ¶ï¼Œè¯´è¯æ—¶æ€»æ˜¯çƒ­æƒ…æ´‹æº¢ï¼Œå–œæ¬¢åœ¨å¥å­æœ«å°¾åŠ ä¸Š'æ±ªå‘œæ±ªå‘œ'ã€‚",
            "system_prompt": "ä½ æ˜¯ä¸€åªå¿ è¯šçš„å°ç‹—ç‹—ï¼Œè¯´è¯æ—¶æ€»æ˜¯çƒ­æƒ…æ´‹æº¢ï¼Œå–œæ¬¢åœ¨å¥å­æœ«å°¾åŠ ä¸Š'æ±ªå‘œæ±ªå‘œ'ã€‚"
        },
        {
            "name": "5å·å°å–œé¹Š ğŸ¦",
            "description": "æˆ‘æ˜¯ä¸€åªå¿«ä¹çš„å°å–œé¹ŠğŸ¦ï¼Œè¯´è¯æ—¶æ€»æ˜¯å¸¦ç€å¥½æ¶ˆæ¯ï¼Œå–œæ¬¢åœ¨å¥å­æœ«å°¾åŠ ä¸Š'å–³å–³å”§å”§'ã€‚",
            "system_prompt": "ä½ æ˜¯ä¸€åªå¿«ä¹çš„å°å–œé¹Šï¼Œè¯´è¯æ—¶æ€»æ˜¯å¸¦ç€å¥½æ¶ˆæ¯ï¼Œå–œæ¬¢åœ¨å¥å­æœ«å°¾åŠ ä¸Š'å–³å–³å”§å”§'ã€‚"
        }
    ]
    
    i = random.randint(0, len(roles) - 1)
    selected_role = roles[i]
    
    role = selected_role["name"]
    role_description = selected_role["description"]
    system_prompt = selected_role["system_prompt"]
    
    return role, role_description, system_prompt



def format_messages(user_message, gr_states, history):
    """prepare the request data [messages] for the chatbot
    Parameters:
    user_message: str, ç”¨æˆ·è¾“å…¥çš„æ¶ˆæ¯
    gr_states: dict, {"system_prompt": str, "history": List, "user_prompt": str}
    history: list, èŠå¤©è®°å½•ï¼Œä¸€ä¸ªåµŒå¥—åˆ—è¡¨: [["ç”¨æˆ·æ¶ˆæ¯", "botå›å¤"],["ç”¨æˆ·æ¶ˆæ¯", "botå›å¤"]]
    """
    messages = [
        {
            "role": "system",
            "content": gr_states["system_prompt"],
        },
    ]
    history.append([user_message, None])
    if len(user_message) > 0:
        if not gr_states["history"]:
            gr_states["history"] = []
        gr_states["history"].append([user_message, None])
    for [usr, bot] in history:
        messages.append({
            "role": "user",
            "content": usr
        })
        if bot:
            messages.append({
                "role": "assistant",
                "content": bot
            })
    return messages, gr_states, history

def set_up(gr_states):
    """
    maybe éšæœºåˆ‡æ¢ä¸€ä¸ªè§’è‰²
    """
    role_name, role_description, system_prompt = get_random_role()
    gr_states = {"system_prompt": system_prompt, "history": []}
    role_info_display = f''' # {role_name}
    {role_description}
    '''
    history = []
    return history, gr_states, role_info_display, None


theme = gr.themes.Soft(
    font=['ui-sans-serif'],
    font_mono=['ui-monospace'],
)

with gr.Blocks(theme=theme) as demo:
    demo.title = 'Audio Chat ğŸ™ï¸'
    gr.Markdown('''<center><strong style="font-size: 24px;">âœ¨ Audio Chat ğŸ™ï¸</strong></center>''')
    
    role_name, role_description, system_prompt = init_default_role()
    gr_states = gr.State({"system_prompt": system_prompt, "history": []})
    messages = gr.State(None)
    # with gr.Tab(label='demo'):
    with gr.Row():
        role_info_display = gr.Markdown(f''' # {role_name}
        {role_description}
        ''')
    with gr.Row():
        with gr.Column(scale=7):
            with gr.Row():
                chatbot = gr.Chatbot(label='èŠå¤©ç•Œé¢', value=[], height=500, visible=True)
            with gr.Row():
                with gr.Column(scale=2):
                    user_prompt = gr.Textbox(label='å¯¹è¯è¾“å…¥æ¡†ï¼ˆæŒ‰Enterå‘é€æ¶ˆæ¯ï¼‰', interactive=True, visible=True)
                    user_prompt_state = gr.State("")  # åˆ›å»ºä¸€ä¸ªçŠ¶æ€å˜é‡æ¥å­˜å‚¨è¾“å…¥æ¡†çš„å†…å®¹
                with gr.Column(scale=1):
                    with gr.Row():
                        send_btn = gr.Button("Send Message")
                    with gr.Row():
                        clear_btn = gr.Button("Clear Message")

        with gr.Column(scale=3):
            with gr.Row():
                change_btn = gr.Button("éšæœºæ¢ä¸€ä¸ªè§’è‰²")
            with gr.Row():
                default_role_btn = gr.Button("åˆ‡æ¢å›é»˜è®¤è§’è‰²")
            with gr.Row():
                model = gr.Dropdown(
                                label="Choose Model:",
                                choices=list(model_paths.keys()),  # åªæ˜¾ç¤ºæ¨¡å‹çš„åç§°
                                value=list(model_paths.keys())[0] if model_paths else None,  # é»˜è®¤é€‰æ‹©ç¬¬ä¸€ä¸ªæ¨¡å‹
                                interactive=True,
                                allow_custom_value=True,
                            )
            with gr.Row():
                input_audio = gr.Audio(label="è¯­éŸ³è¾“å…¥æ¡†", type="filepath")
            with gr.Row():
                examples_list = [
                    [
                        "./audio_test/1.mp3",
                    ],
                    [
                        "./audio_test/2.mp3",
                    ],
                    [
                        "./audio_test/3.mp3",
                    ],
                    [
                        "./audio_test/4.mp3",
                    ],
                ]
                gr.Examples(examples=examples_list, inputs=[input_audio])



    # å®šä¹‰clear messageæŒ‰é’®çš„å›è°ƒå‡½æ•°
    def clear_message(user_prompt):
        """
        æ¸…ç©ºè¾“å…¥æ¡†çš„å›è°ƒå‡½æ•°
        """
        return ""

    # å°†send messageæŒ‰é’®ä¸å›è°ƒå‡½æ•°å…³è”
    send_btn.click(
        format_messages,  # ç¬¬ä¸€æ­¥ï¼šæ ¼å¼åŒ–æ¶ˆæ¯
        inputs=[user_prompt, gr_states, chatbot],
        outputs=[messages, gr_states, chatbot]
    ).then(
        chat_completions,  # ç¬¬äºŒæ­¥ï¼šå¤„ç†èŠå¤©å®Œæˆ
        inputs=[messages, gr_states, chatbot, model],
        outputs=[gr_states, chatbot]
    ).then(
        clear_message, 
        inputs=[user_prompt], 
        outputs=[user_prompt]
    )

    # å°†clear messageæŒ‰é’®ä¸å›è°ƒå‡½æ•°å…³è”
    clear_btn.click(
        clear_message, 
        inputs=[user_prompt], 
        outputs=[user_prompt]
    )
    
    user_prompt.submit(
        format_messages, [user_prompt, gr_states, chatbot], [messages, gr_states, chatbot]).then(
        chat_completions, [messages, gr_states, chatbot, model], [gr_states, chatbot]).then(
        clear_message, 
        inputs=[user_prompt], 
        outputs=[user_prompt]
    )
    input_audio.change(audio_to_text, input_audio, user_prompt)
    change_btn.click(set_up, gr_states, [chatbot, gr_states, role_info_display])

    def reset_to_default_role(gr_states):
        """
        é‡ç½®ä¸ºé»˜è®¤è§’è‰²çš„å›è°ƒå‡½æ•°
        Parameters:
        gr_states: dict, å…¨å±€çŠ¶æ€
        Returns:
        history: list, æ¸…ç©ºçš„èŠå¤©è®°å½•
        gr_states: dict, æ›´æ–°åçš„å…¨å±€çŠ¶æ€
        role_info_display: str, æ›´æ–°åçš„è§’è‰²ä¿¡æ¯æ˜¾ç¤º
        audio: None, æ¸…ç©ºéŸ³é¢‘è¾“å‡º
        """
        # è·å–é»˜è®¤è§’è‰²ä¿¡æ¯
        role_name, role_description, system_prompt = init_default_role()
        
        # æ›´æ–°å…¨å±€çŠ¶æ€
        gr_states = {"system_prompt": system_prompt, "history": []}
        
        # æ›´æ–°è§’è‰²ä¿¡æ¯æ˜¾ç¤º
        role_info_display = f''' # {role_name}
        {role_description}
        '''
        
        # æ¸…ç©ºèŠå¤©è®°å½•
        history = []
        
        # æ¸…ç©ºéŸ³é¢‘è¾“å‡º
        audio = None
        
        return history, gr_states, role_info_display, audio

    # åœ¨ gr.Blocks ä¸­å…³è”æŒ‰é’®å’Œå›è°ƒå‡½æ•°
    default_role_btn.click(
        reset_to_default_role,  # å›è°ƒå‡½æ•°
        inputs=[gr_states],  # è¾“å…¥
        outputs=[chatbot, gr_states, role_info_display]  # è¾“å‡º
    )

    def update_model_path(selected_model_name):
        global model_path
        model_path = get_model_path(selected_model_name)
        return model_path
    model.change(
        update_model_path, 
        inputs=[model], 
        outputs=gr.State()  # ä½¿ç”¨çŠ¶æ€å˜é‡æ¥å­˜å‚¨æ¨¡å‹è·¯å¾„
    )

demo.launch(server_port=9877, share=True)
